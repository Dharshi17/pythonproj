#import library packages
import pandas as p
import matplotlib.pyplot as plt
import seaborn as s
import numpy as n
import warnings
warnings.filterwarnings('ignore')
data = p.read_csv('data1.csv')
data.head()
data.shape
data.info()
data.columns
data['CHECKINGSTATUS'].unique()
data['LOANDURATION'].unique()
data['CREDITHISTORY'].unique()
data['LOANPURPOSE'].unique()
data['LOANAMOUNT'].nunique()
data['EXISTINGSAVINGS'].unique()
data['EMPLOYMENTDURATION'].unique()
data['INSTALLMENTPERCENT'].unique()
data['SEX'].unique()
data['OTHERSONLOAN'].unique()
data['CURRENTRESIDENCEDURATION'].unique()
data['OWNSPROPERTY'].unique()
data['AGE'].unique()
data['INSTALLMENTPLANS'].unique()
data['HOUSING'].unique()
data['EXISTINGCREDITSCOUNT'].unique()
data['JOB'].unique()
data['DEPENDENTS'].unique()
data['TELEPHONE'].unique()
data['FOREIGNWORKER'].unique()
data['RISK'].unique()
df=data.dropna()
df.shape
df.columns
del df['CUSTOMERID']
del df['Unnamed: 0']
df.columns
from sklearn.preprocessing import LabelEncoder
label_columns= ['CHECKINGSTATUS','CREDITHISTORY',
       'LOANPURPOSE','EXISTINGSAVINGS', 'EMPLOYMENTDURATION',
 'SEX', 'OTHERSONLOAN','OWNSPROPERTY','INSTALLMENTPLANS','HOUSING','JOB','TELEPHONE',
       'FOREIGNWORKER', 'RISK']
lebel_data = LabelEncoder()
for i in label_columns:
    df[i] = lebel_data.fit_transform(df[i]).astype(int) 
df.head()
df.info()
#preprocessing, split test and dataset, split response variable
X = df.drop(labels='RISK', axis=1)
#Response variable
y = df.loc[:,'RISK']    
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10, stratify=y)
print("Number of training dataset: ", len(X_train))
print("Number of test dataset: ", len(X_test))
print("Total number of dataset: ", len(X_train)+len(X_test))
#According to the cross-validated MCC scores, the random forest is the best-performing model, so now let's evaluate its performance on the test set.
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

DT= RandomForestClassifier(max_depth=5)

DT.fit(X_train,y_train)

predictDT = DT.predict(X_test)

print("")
print('Classification report of RandomForestClassifier Results:')
print("")
print(classification_report(y_test,predictDT))


print("")
print('accuracy_score of RandomForestClassifier Results:')
print("")
print(accuracy_score(y_test,predictDT)*100)


print("")
cm1=confusion_matrix(y_test,predictDT)
print('Confusion Matrix result of RandomForestClassifier is:\n',cm1)
print("")
sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Sensitivity : ', sensitivity1 )
print("")
specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Specificity : ', specificity1)
print("")

accuracy = cross_val_score(DT, X, y, scoring='accuracy')
print('Cross validation test results of accuracy:')
print(accuracy)
#get the mean of each fold 
print("")
print("Accuracy result of RandomForestClassifier is:",accuracy.mean() * 100)
LR=accuracy.mean() * 100
#checking the feature improtance in the model
plt.figure(figsize=(9,7))
n_features = X_train.shape[1]
plt.barh(range(n_features), DT.feature_importances_, align='center')
plt.yticks(n.arange(n_features), X_train.columns)
plt.xlabel("Feature importance")
plt.ylabel("Feature")
plt.show()
import joblib
joblib.dump(DT,'rf.pkl')

